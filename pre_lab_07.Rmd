---
title: "pre_lab_07.Rmd"
author: "Devon Milley"
date: "3/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 16

In the last chapter, we demonstrated a fairly straightforward example of web scraping to grab a list of NAICS industry sector codes from the BLS website.  

We're going to graduate to a more challenging example, one that will help us gather information about the number of employees in each industry sector.

What makes this more challenging?  Well, the information we need is all contained on multiple pages, one page per sector. We need to write code to visit each page, and then merge them into a single data frame.
This is challenging stuff, so don't feel dissuaded if it all doesn't click the first time through.  Like many things, web scraping is something that gets easier with lots of practice.

First we start with libraries, as we always do.

### Task 1: Load packages
**Task** Run the following code to load packages.

```{r}
library(rvest)
library(tidyverse)
library(janitor)
```


We don't just want it for mining.  We want it for all sectors!  

But we'll start by writing code just to get it from this one sector page, then modify that code to get it from every sector's page

First, let's define the URL of the page we want to get the information from.

### Task 6: Store URL
**Task** Run the following code to store the URL.

```{r}

# Define url of the page we want to get
umterps_cal_url <- "https://umterps.com/calendar"

```

Next, let's read in the html of that page, and store it as an object called employment_info.

### Task 7: Run code to read in html
**Task** Run the following code to read in the html. Briefly describe the output that appears below the codeblock.
**Answer** a list containing a head tag and a body tag

```{r}

# Define url of the page we want to get
umterps_cal_url <- "https://umterps.com/calendar"

# Get employment html
umterps_calendar <- umterps_cal_url %>%
  read_html()  

# Display it so we can see what it looks like
umterps_calendar

```


### Task 9: Run code to get html_element with info we need
**Task** Run the following code to get html_element with info we need. Briefly describe the output that appears below the codeblock.
**Answer** a table tag that has a nested list containing caption, thead, tbody and tfoot tags.

```{r}

# Define url of the page we want to get
umterps_cal_url <- "https://umterps.com/calendar"

# Get employment html page and select only the table with employment information
umterps_calendar <- umterps_cal_url %>%
  read_html() %>%
  html_element(xpath = '//*[@class="sidearm-calendar-table-cell"]')

# Display it so we can see what it looks like
umterps_calendar
```

We've now isolated the table on the page that contains the information we need, and gotten rid of everything else.

From here, we can use the html_tables() function to transform it from messy html code to a proper dataframe.

### Task 10: Run code to convert to table
**Task** Run the following code to convert to table. Briefly describe the output that appears below the codeblock.
**Answer** a tibble of the table we wanted from the website

```{r}

# Define url of the page we want to get
umterps_cal_url <- "https://umterps.com/calendar"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
umterps_calendar <- umterps_cal_url %>%
  read_html() %>%
  html_element(xpath = '//*[@class="sidearm-calendar-table"]')
  html_table()

# Display it so we can see what it looks like
umterps_calendar
```

Now we have a proper dataframe of 6 rows and 6 columns.  

It has much more information than we need, so let's clean it up to isolate only the "Employment, all employees (seasonally adjusted)" value for Nov. 2021.

Use clean_names() to standardize the column names, use slice() to keep only the second row, and use select() to keep two columns data_series and nov_2021.

### Task 11: Run code to keep row 2 and light cleaning
**Task** Run the following code to keep row 2 and light cleaning. Briefly describe the output that appears below the codeblock.
**Answer** a tibble containing the employment catefory and the number for November 2021.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag21emp1"]') %>%
  html_table()

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
  clean_names() %>%
  slice(2) %>%
  select(data_series, nov_2021)

# Display it so we can see what it looks like
employment_info
```

Okay, so we've successfully obtained the employment numbers for one of our sectors. That's great.

But remember our original charge: to get a table with employment numbers for ALL sectors, not just one.

This is a little tricky, because, remember, the information for each sector is on a different page!

The info for mining is on this page: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm).

### Task 12: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer**

The info for construction is on this page: [https://www.bls.gov/iag/tgs/iag23.htm](https://www.bls.gov/iag/tgs/iag23.htm).

### Task 13: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer**

We have 20 sectors to get through.  

We could get the info we need by copying the codeblock we just wrote 20 times, and change the url at the top each time.  

But that's not a great approach.  

What if we needed to change the code? We'd need to change it 20 times!
In programming, there's a principle called "DRY" which stands for "Don't Repeat Yourself".  If you find yourself copying the same code over and over again, with minor changes, it's better to find a way to avoid that.

## Using for loops

Fortunately, there's a programming paradigm called "iteration" that is helpful here, using a method called a "for loop".

Every programming language has its own version of a "for loop", and R is no different.

A "for loop" says: "let's take a list of things, and do the same thing to each item on that list."   

Let's look at a very simple example to help illustrate the values of for loops.

We're going to write code to print out 10 industry sectors.  

First, let's do it the repetitive way.  We're writing the same print function over and over, just changing the sector name each time.

### Task 14: Run code to print sectors
**Task** Run the following code to print sectors. Briefly describe the output that appears below the codeblock.
**Answer** Each sector description, printed one after another
```{r}

print("Agriculture, Forestry, Fishing and Hunting")
print("Mining, Quarrying, and Oil and Gas Extraction")
print("Utilities")
print("Construction")
print("Manufacturing")
print("Wholesale Trade")
print("Retail Trade")
print("Transportation and Warehousing")
print("Information")
print("Finance and Insurance")

```

We repeated print() 10 times, with minor modifications each time.  Lots of repetition, which we seek to avoid if possible.

Now let's look at how we might do that a little more efficiently with a "for loop."

First let's make a list of sectors, and save it as an object called "list_of_sectors." The c() function tells R that we're making a list.

### Task 15: Run code to make list of sectors
**Task** Run the following code to make list of sectors. Briefly describe the output that appears below the codeblock.
**Answer** there is no output
```{r}
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

```

And now let's write a "for loop" to print out sector on that list.

### Task 16: Run code to use for loop to print sectors
**Task** Run the following code to use a for loop to print list of sectors. Briefly describe the output that appears below the codeblock.
**Answer** each sector description, printed one after another
```{r}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# Make a for loop and run it
for (sector in list_of_sectors) {
  print(sector)
}


```

That's many fewer lines of code.  Let's break down what we just saw, starting with for `(sector in list_of sectors)`.

The information inside the parentheses tells R what list to use -- list_of_sectors -- and how to identify list elements later on -- sector.

It's important that the thing on the right side of "in" use the exact name of the list we want to loop through -- in this case "list_of_sectors".

If we try to feed it something different -- say "sector_list" -- it won't work, because our actual list is called something else -- "list_of_sectors". This code throws an error.

### Task 17: Run code to use for loop to print sectors that errors
**Task** Run the following code to use a for loop to print list of sectors that errors. Briefly describe the output that appears below the codeblock.
**Answer** Error: it can't find sector_list because that's not the name of the list we created

```{r, error=TRUE}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop that refers to a list that doesn't exist!  
for (sector in sector_list) {
  print(sector)
}


```

The name on the left side of "in" -- the word we're assigning to represent each element -- is totally arbitrary.  

We could use any character string, even something simple like "x".  

What matters is that we use the same character string inside of the curly braces {}, the section of the "for loop" that tells R what to do to each element -- in this case, print it out.    

To illustrate this, note that the code works just fine if we change it to say this:

### Task 18: Run code to use for loop to print sectors
**Task** Run the following code to use a for loop to print list of sectors. Briefly describe the output that appears below the codeblock.
**Answer** the sector descriptions, printed one after another
```{r}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop with x that stands in for each element in our list, instead of sector
for (x in list_of_sectors) {
  print(x)
}


```

But it does NOT work if we call each element one thing -- x -- in the first line of our "for loop", and use a different name to refer to it inside of the curly braces.

In this code below, it has no idea what we mean by "sector_name", because we haven't defined that anywhere.  

### Task 19: Run code to use for loop to print sectors that errors
**Task** Run the following code to use a for loop to print list of sectors. Briefly describe the output that appears below the codeblock.
**Answer** error: couldn't find the sector_name object because that's not what we named it in the for loop
```{r, error=TRUE}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop that includes instructions that refer to a variable that doesn't exist.
for (x in list_of_sectors) {
  print(sector_name)
}


```

We can also write for loops to iterate over a range of numbers, instead of a list of characters.  The syntax is a little different.

The code below says: "for each number in a range of numbers from 1 to 10, print the number."  

### Task 20: Run code to use for loop to print 1 to 10
**Task** Run the following code to use a for loop to print 1 to 10. Briefly describe the output that appears below the codeblock.
**Answer** the numbers 1 through 10, printed one after another
```{r}
# For loop that includes instructions that refer to a variable that doesn't exist.
for (number in 1:10) {
  print(number)
}
```

Here's a minor variation on that approach that we'll make use of below.  

Instead of giving the for loop an explicit number range, like 1:10, we can tell it to use 1 to "the number of rows in a dataframe" as our list of things to loop through.

Remember the naics_industry dataframe we loaded first? It has 20 rows.
### Task 21: Run code to display naics_industry
**Task** Run the following code to display naics_industry. Briefly describe the output that appears below the codeblock.
**Answer** a tibble of the cleaned sector table 
```{r}

naics_industry

```

We can use that information in our for loop by using the nrow() function, which calculates the number of rows in a dataframe.  Here's a quick demonstration of how that works.

### Task 22: Run code to display number of rows in naics_industry
**Task** Run the following code to display number of rows naics_industry. Briefly describe the output that appears below the codeblock.
**Answer** the numbers of rows in naics_industry

```{r}
nrow(naics_industry)
```

To put it all together, the code below says "make a list of numbers that starts at 1 and ends at the number of rows in the naics_industry dataframe (which is 20), then print out each of these numbers."

### Task 23: Run code to print each row number in naics_industry
**Task** Run the following code to to print each row number in naics_industry. Briefly describe the output that appears below the codeblock.
**Answer** each row number in naics_industry, printed out one after another

```{r}
# For loop that includes instructions that refer to a variable that doesn't exist.
for (row_number in 1:nrow(naics_industry)) {
  print(row_number)
}
```

These were basic examples of how "for loops" work.  Next, we'll learn to apply "for loops" to efficentily extract information from multiple web pages.

## Looping and rvest

First, let's look at the codeblock we wrote earlier to extract the number of employees in the mining sector.

### Task 24: Run code to load table from earlier
**Task** Run the following code to load table from earlier. Briefly describe the output that appears below the codeblock.
**Answer** prints out employment for November 2021

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag21emp1"]') %>%
  html_table()

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
  clean_names() %>%
  slice(2) %>%
  select(data_series, nov_2021)

# Display it so we can see what it looks like
employment_info
```
This contains all the steps we needed to extract the information from one sector page. We're now going to modify this function so we can use it to extract information from each sector page, writing code that keeps us from repeating ourselves too much.

First, we need to build a list of URLs to loop through in a "for loop." We can do that using the dataframe we made in the last chapter.
### Task 25: Run code to load the table from the last chapter
**Task** Run the following code to load table from last chapter. Briefly describe the output that appears below the codeblock.
**Answer** a cleaned tibble of the sector and secotr description table
```{r}
# Define url of page we want to scrape

naics_url <- "https://www.bls.gov/ces/naics/"

# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] %>%
  clean_names() %>%
  slice(-21)

# show the dataframe
naics_industry

```

This gives us the sector code and name for each industry.

Now let's have a look at the URLs for a few of the pages we want to grab data from.

* Mining, Quarrying, and Oil and Gas Extraction: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm).
* Utilities: [https://www.bls.gov/iag/tgs/iag22.htm](https://www.bls.gov/iag/tgs/iag22.htm).
* Construction: [https://www.bls.gov/iag/tgs/iag23.htm](https://www.bls.gov/iag/tgs/iag23.htm)

Notice a pattern?

They all start with "https://www.bls.gov/iag/tgs/iag".  The next bit of information is different for each one; with the two-digit sector code for each sector.  The remainder is identical in all three links, ".htm".

Because they're all the same, we can use the information in the dataframe we just loaded to make all the URLs we need.

We're going to use mutate() and paste0() to concatenate (mash together) the things that stay constant in every url (the beginning and end) with the things that are different (the sector number, stored in the column called sector).

### Task 26: Run code to build url
**Task** Run the following code to build url. Briefly describe the output that appears below the codeblock.
**Answer** a tibble with the sector number and sector description and a url to where to to get the employment information we want for each sector 

```{r}

# Make a column with URL for each sector.
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm"))

# Display it
naics_industry
```

While we're at it, we're going to use the same method to programatically build the "xpath" for the table on each sector page.  

Recall that when we wrote our function that got information from just the mining page, the xpath targeted an element with an ID of "iag21emp1".  Why 21? That's the sector code for mining.  

If we look for that exact element ID on other sector pages, we won't find it! That's because it's different for each page.

On the Utilities page (sector code 22), the ID for the table we want is "iag22emp1".  On the Construction page (sector code 23), it's "iag23emp1". We can also build this programatically, because it follows a predictable pattern.

### Task 27: Run code to build id
**Task** Run the following code to build id. Briefly describe the output that appears below the codeblock.
**Answer** a tibble of the sector and sector description and the unique part of the employment url that takes us to the right sector's page

```{r}

# Make a column with URL and xpath ID for each sector
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
  mutate(sector_xpath_id =paste0("iag",sector,"emp1"))

# Display it
naics_industry
```

Lastly, we're going to use filter to remove the "Public Administration" sector, because there's no page for it. We'll have to get that information some other way.

### Task 28: Run code to filter table
**Task** Run the following code to filter table. Briefly describe the output that appears below the codeblock.
**Answer** a tibble with the sectore, sector description, employment information url and sector-unique part of the url

```{r}

# Make a column with URL and xpath ID for each sector, remove the Public Administration sector
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
  mutate(sector_xpath_id =paste0("iag",sector,"emp1")) %>%
  filter(description != "Public Administration")
# Display it
naics_industry
```

We're left with a dataframe of 19 rows and 4 columns. It now contains everything we need.

Next, we'll construct a "for loop" to extract the info we need from each page. We're going to build it up step-by-step, beginning with the the basic elements of our "for loop".

The codeblock below says: "Make a list with the row numbers from 1 to the number of rows in our naics_industry dataframe (which is 19). Then, for each element of that list (1, 2, 3, 4, 5 and so on up to 19), use slice() to keep only the one row that matches that number and save this newly created dataframe as each_row_df. Print out the dataframe. Then go to the next element on the list and do the same thing.  Keep doing that until we hit number 19, then stop."   

We get 19 dataframes, each with one row, one for each sector.

### Task 29: Run code to run for loop and keep one row
**Task** Run the following for loop and keep one row. Briefly describe the output that appears below the codeblock.
**Answer** 19 tibbles, one for each sector

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(each_row_df)

}
```

We're almost to the part where we can go out and fetch the html we need. Before we do that, let's store as part of our loop an object called "url", which contains the URL of the page for each sector.

The syntax with the dollar sign is a little funky, but "each_row_df$sector_url" says "from the each_row_df dataframe, grab the information in the sector_url column." Because the column has only one row, there's one value.

We're going to do something simliar with the xpath for our employment table by using the information in the sector_xpath_id column.

That code also looks a little unwieldly.  Recall that the xpath for the mining industry was `'//*[@id="iag22emp1"]'`.  

In the code below, we're building the xpath dynamically by pasting together the parts that stay the same for each xpath -- `'//*[@id="'` and `'"]'` -- and the parts that change for each sector, pulled from the xpath_sector_id column.

To see how this is working, we're going to edit our print statement at the end a bit, printing the row_number and the dynamically created url and xpath.

### Task 30: Run code to run for loop to store url and xpath value
**Task** Run the following for loop to store url and xpath value. Briefly describe the output that appears below the codeblock.
**Answer** each sector-specific employment url and its xpath, printed out one after another
```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(paste0("ROW NUMBER:", row_number," URL: ",url," XPATH:",xpath_employment_table))

}
```

Armed with the URL and xpath for each sector web page, we can now go out and get the employment table for each sector.

We'll read in the html from the url we just stored; extract the table that has the xpath ID we just created; and then transform the html table code into a proper dataframe.

The dataframe is hidden inside  a nested list, which we'll have to extract in the next step.

So, when you run this code, it will print out 19 dataframes inside of nested lists, each containing one dataframe.

### Task 31: Run code to run for loop to get tables
**Task** Run the following for loop to get tables. Briefly describe the output that appears below the codeblock.
**Answer** 20 tibbles of employment information, one for each sector

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table(). The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
In this next step, we use employment_info <- employment_info[[1]]  to extract each dataframe from the nested list. Then we'll tidy up the dataframe a bit. We'll use the get rid of all the information we don't need in the table, by using slice() to keep only the second row. We'll also standardize the column names with clean_names().

### Task 32: Run code to run for loop to clean up tables
**Task** Run the following for loop to clean up tables. Briefly describe the output that appears below the codeblock.
**Answer** a cleaned verson of those tibbles that lowercased the headers, and only looks at the "employment, all employyes (seasonally adjusted)" row

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row;
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
We now have 19 dataframes, each containing one row each and two columns, one of which is the employment number for a given sector for jun_2021. But we're missing information about what industry sector these employment numbers represent.

We can add that back in by using bind_cols() to reconnect the each_row_df, which contains the sector code and the sector name.

### Task 33: Run code to run for loop to add in data
**Task** Run the following for loop to add in data. Briefly describe the output that appears below the codeblock.
**Answer** 19 tibbles that combined the employment information table with the sectors table

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
Then we'll do a little bit of cleaning.

Let's use parse_number() to remove the comma from the jun_2021 number and convert it from a character to number. We'll use rename() to make the jun_2021 column name a little more descriptive. And then we'll use select() to keep only the columns we want to keep -- the sector number, the sector name, and the jun_2021 employment number.

### Task 34: Run code to run for loop to clean up tables
**Task** Run the following for loop to clean up tables. Briefly describe the output that appears below the codeblock.
**Answer** 19 tibbles containing the sector and sector description and number of employees for November 2021

```{r}

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jun_2021 column into a proper number, and rename it.  Then select only three columns we need.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df) %>%
      mutate(nov_2021 = parse_number(nov_2021)) %>%
      rename(nov_2021_employees = nov_2021) %>%
      select(sector,description,nov_2021_employees)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}

```


We're getting very close to the finished table we showed at the beginning.  

But right now, each bit of sector information is separated between 19 different dataframes.  

We want them in one dataframe.  

We can fix this by creating an empty dataframe called "employment_by_sector_all" using tibble(), placing it before our "for loop".

And inside our "for loop" at the end, we'll bind each employment_info dataframe to the newly created empty dataframe.  

### Task 35: Run code to run for loop to combine tables into a single table
**Task** Run the following for loop combine tables into a single table. Briefly describe the output that appears below the codeblock.
**Answer** a tibble of all the sectors and sector descriptions and their respective employment information for November 2021

```{r}

# Create an empty dataframe to hold results
employment_by_sector_all <- tibble()

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jun_2021 column into a proper number, and rename it.  Then select only three columns we need.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df) %>%
      mutate(nov_2021 = parse_number(nov_2021)) %>%
      rename(nov_2021_employees = nov_2021) %>%
      select(sector,description,nov_2021_employees)

    # Bind each individual employment info table to our employment_by_sector_all dataframe
    employment_by_sector_all <- employment_by_sector_all %>%
      bind_rows(employment_info)

}

# Display the completed dataframe
employment_by_sector_all
```

Ta da! The end result is a nice tidy dataframe with the number of employees in June 2021 for each sector.

It's always a good idea to spot check the results, especially any values that look suspiciously high or low.

The value for "Agriculture, Forestry, Fishing and Hunting" seems suspiciously low, compared with the other values.  

Let's figure out why.  

Here's the table on the mining sector page: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm)

### Task 36: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer** Employment information for the mining sector

### Task 37: Load image
**Task** Run the following code to display an image showing what you should see on the web page.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest3.png"))
```
And here's the table for the agriculture sector.

[https://www.bls.gov/iag/tgs/iag11.htm](https://www.bls.gov/iag/tgs/iag11.htm)

### Task 38: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer** employment information for the agriculture sector.

### Task 39: Load image
**Task** Run the following code to display an image showing what you should see on the web page.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest4.png"))
```
Unlike mining -- and every other sector page (I checked each page) -- the agriculture page is structured differently.  

In the second row of this table, it has the unemployment rate. Nowhere on the page can we find information on the number of employees.  We would need to do additional research to track down a valid number if we plan on using this table, but for now we're going to replace it with an NA using na_if.

### Task 40: Run code to remove agriculture from table
**Task** Run the following code to remove agriculture from table. Briefly describe the output that appears below the codeblock.
**Answer** This removes the odd number and replaces it with NA from the November 2021 employment in the agriculture sector

```{r}
# remove the suspicious value for agriculture.
employment_by_sector_all <- employment_by_sector_all %>%
  mutate(nov_2021_employees = na_if(nov_2021_employees,5.4))

# display it
employment_by_sector_all
```

And we're done.  

A note about advanced scraping -- every site is different. Every time you want to scrape a site, you'll be puzzling over different problems. But the steps remain the same: find a pattern, exploit it, clean the data on the fly and put it into a place to store it.

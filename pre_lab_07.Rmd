---
title: "pre_lab_07.Rmd"
author: "Devon Milley"
date: "3/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 16

In the last chapter, we demonstrated a fairly straightforward example of web scraping to grab a list of NAICS industry sector codes from the BLS website.  

We're going to graduate to a more challenging example, one that will help us gather information about the number of employees in each industry sector.

What makes this more challenging?  Well, the information we need is all contained on multiple pages, one page per sector. We need to write code to visit each page, and then merge them into a single data frame.
This is challenging stuff, so don't feel dissuaded if it all doesn't click the first time through.  Like many things, web scraping is something that gets easier with lots of practice.

First we start with libraries, as we always do.

### Task 1: Load packages
**Task** Run the following code to load packages.

```{r}
library(rvest)
library(tidyverse)
library(janitor)
```


We don't just want it for mining.  We want it for all sectors!  

But we'll start by writing code just to get it from this one sector page, then modify that code to get it from every sector's page

First, let's define the URL of the page we want to get the information from.

### Task 6: Store URL
**Task** Run the following code to store the URL.

```{r}

# Define url of the page we want to get
umterps_cal_url <- "https://umterps.com/calendar"

```

Next, let's read in the html of that page, and store it as an object called employment_info.

### Task 7: Run code to read in html
**Task** Run the following code to read in the html. Briefly describe the output that appears below the codeblock.
**Answer** a list containing a head tag and a body tag

```{r}

# Define url of the page we want to get
umterps_cal_url <- "https://umterps.com/calendar"

# Get employment html
umterps_calendar <- umterps_cal_url %>%
  read_html()  

# Display it so we can see what it looks like
umterps_calendar

```


### Task 9: Run code to get html_element with info we need
**Task** Run the following code to get html_element with info we need. Briefly describe the output that appears below the codeblock.
**Answer** a table tag that has a nested list containing caption, thead, tbody and tfoot tags.

```{r}

# Define url of the page we want to get
umterps_cal_url <- "https://umterps.com/calendar?date=3/9/2022&vtype=list"

# Get employment html page and select only the table with employment information
umterps_calendar <- umterps_cal_url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="sidearm-calendar-schedule"]')

# Display it so we can see what it looks like
umterps_calendar
```

We've now isolated the table on the page that contains the information we need, and gotten rid of everything else.

From here, we can use the html_tables() function to transform it from messy html code to a proper dataframe.

### Task 10: Run code to convert to table
**Task** Run the following code to convert to table. Briefly describe the output that appears below the codeblock.
**Answer** a tibble of the table we wanted from the website

```{r}

# Define url of the page we want to get
umterps_cal_url <- "https://umterps.com/calendar?date=3/9/2022&vtype=list"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
umterps_calendar <- umterps_cal_url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="sidearm-calendar-schedule"]')
  html_table()

# Display it so we can see what it looks like
umterps_calendar
```

Now we have a proper dataframe of 6 rows and 6 columns.  

It has much more information than we need, so let's clean it up to isolate only the "Employment, all employees (seasonally adjusted)" value for Nov. 2021.

Use clean_names() to standardize the column names, use slice() to keep only the second row, and use select() to keep two columns data_series and nov_2021.

### Task 11: Run code to keep row 2 and light cleaning
**Task** Run the following code to keep row 2 and light cleaning. Briefly describe the output that appears below the codeblock.
**Answer** a tibble containing the employment catefory and the number for November 2021.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag21emp1"]') %>%
  html_table()

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
  clean_names() %>%
  slice(2) %>%
  select(data_series, nov_2021)

# Display it so we can see what it looks like
employment_info
```



### Task 24: Run code to load table from earlier
**Task** Run the following code to load table from earlier. Briefly describe the output that appears below the codeblock.
**Answer** prints out employment for November 2021

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag21emp1"]') %>%
  html_table()

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
  clean_names() %>%
  slice(2) %>%
  select(data_series, nov_2021)

# Display it so we can see what it looks like
employment_info
```
This contains all the steps we needed to extract the information from one sector page. We're now going to modify this function so we can use it to extract information from each sector page, writing code that keeps us from repeating ourselves too much.

First, we need to build a list of URLs to loop through in a "for loop." We can do that using the dataframe we made in the last chapter.
### Task 25: Run code to load the table from the last chapter
**Task** Run the following code to load table from last chapter. Briefly describe the output that appears below the codeblock.
**Answer** a cleaned tibble of the sector and secotr description table
```{r}
# Define url of page we want to scrape

naics_url <- "https://www.bls.gov/ces/naics/"

# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] %>%
  clean_names() %>%
  slice(-21)

# show the dataframe
naics_industry

```

This gives us the sector code and name for each industry.

Now let's have a look at the URLs for a few of the pages we want to grab data from.

* Mining, Quarrying, and Oil and Gas Extraction: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm).
* Utilities: [https://www.bls.gov/iag/tgs/iag22.htm](https://www.bls.gov/iag/tgs/iag22.htm).
* Construction: [https://www.bls.gov/iag/tgs/iag23.htm](https://www.bls.gov/iag/tgs/iag23.htm)

Notice a pattern?

They all start with "https://www.bls.gov/iag/tgs/iag".  The next bit of information is different for each one; with the two-digit sector code for each sector.  The remainder is identical in all three links, ".htm".

Because they're all the same, we can use the information in the dataframe we just loaded to make all the URLs we need.

We're going to use mutate() and paste0() to concatenate (mash together) the things that stay constant in every url (the beginning and end) with the things that are different (the sector number, stored in the column called sector).

### Task 26: Run code to build url
**Task** Run the following code to build url. Briefly describe the output that appears below the codeblock.
**Answer** a tibble with the sector number and sector description and a url to where to to get the employment information we want for each sector 

```{r}

# Make a column with URL for each sector.
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm"))

# Display it
naics_industry
```

While we're at it, we're going to use the same method to programatically build the "xpath" for the table on each sector page.  

Recall that when we wrote our function that got information from just the mining page, the xpath targeted an element with an ID of "iag21emp1".  Why 21? That's the sector code for mining.  

If we look for that exact element ID on other sector pages, we won't find it! That's because it's different for each page.

On the Utilities page (sector code 22), the ID for the table we want is "iag22emp1".  On the Construction page (sector code 23), it's "iag23emp1". We can also build this programatically, because it follows a predictable pattern.

### Task 27: Run code to build id
**Task** Run the following code to build id. Briefly describe the output that appears below the codeblock.
**Answer** a tibble of the sector and sector description and the unique part of the employment url that takes us to the right sector's page

```{r}

# Make a column with URL and xpath ID for each sector
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
  mutate(sector_xpath_id =paste0("iag",sector,"emp1"))

# Display it
naics_industry
```

Lastly, we're going to use filter to remove the "Public Administration" sector, because there's no page for it. We'll have to get that information some other way.

### Task 28: Run code to filter table
**Task** Run the following code to filter table. Briefly describe the output that appears below the codeblock.
**Answer** a tibble with the sectore, sector description, employment information url and sector-unique part of the url

```{r}

# Make a column with URL and xpath ID for each sector, remove the Public Administration sector
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
  mutate(sector_xpath_id =paste0("iag",sector,"emp1")) %>%
  filter(description != "Public Administration")
# Display it
naics_industry
```

We're left with a dataframe of 19 rows and 4 columns. It now contains everything we need.

Next, we'll construct a "for loop" to extract the info we need from each page. We're going to build it up step-by-step, beginning with the the basic elements of our "for loop".

The codeblock below says: "Make a list with the row numbers from 1 to the number of rows in our naics_industry dataframe (which is 19). Then, for each element of that list (1, 2, 3, 4, 5 and so on up to 19), use slice() to keep only the one row that matches that number and save this newly created dataframe as each_row_df. Print out the dataframe. Then go to the next element on the list and do the same thing.  Keep doing that until we hit number 19, then stop."   

We get 19 dataframes, each with one row, one for each sector.

### Task 29: Run code to run for loop and keep one row
**Task** Run the following for loop and keep one row. Briefly describe the output that appears below the codeblock.
**Answer** 19 tibbles, one for each sector

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(each_row_df)

}
```

We're almost to the part where we can go out and fetch the html we need. Before we do that, let's store as part of our loop an object called "url", which contains the URL of the page for each sector.

The syntax with the dollar sign is a little funky, but "each_row_df$sector_url" says "from the each_row_df dataframe, grab the information in the sector_url column." Because the column has only one row, there's one value.

We're going to do something simliar with the xpath for our employment table by using the information in the sector_xpath_id column.

That code also looks a little unwieldly.  Recall that the xpath for the mining industry was `'//*[@id="iag22emp1"]'`.  

In the code below, we're building the xpath dynamically by pasting together the parts that stay the same for each xpath -- `'//*[@id="'` and `'"]'` -- and the parts that change for each sector, pulled from the xpath_sector_id column.

To see how this is working, we're going to edit our print statement at the end a bit, printing the row_number and the dynamically created url and xpath.

### Task 30: Run code to run for loop to store url and xpath value
**Task** Run the following for loop to store url and xpath value. Briefly describe the output that appears below the codeblock.
**Answer** each sector-specific employment url and its xpath, printed out one after another
```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(paste0("ROW NUMBER:", row_number," URL: ",url," XPATH:",xpath_employment_table))

}
```

Armed with the URL and xpath for each sector web page, we can now go out and get the employment table for each sector.

We'll read in the html from the url we just stored; extract the table that has the xpath ID we just created; and then transform the html table code into a proper dataframe.

The dataframe is hidden inside  a nested list, which we'll have to extract in the next step.

So, when you run this code, it will print out 19 dataframes inside of nested lists, each containing one dataframe.

### Task 31: Run code to run for loop to get tables
**Task** Run the following for loop to get tables. Briefly describe the output that appears below the codeblock.
**Answer** 20 tibbles of employment information, one for each sector

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table(). The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
In this next step, we use employment_info <- employment_info[[1]]  to extract each dataframe from the nested list. Then we'll tidy up the dataframe a bit. We'll use the get rid of all the information we don't need in the table, by using slice() to keep only the second row. We'll also standardize the column names with clean_names().

### Task 32: Run code to run for loop to clean up tables
**Task** Run the following for loop to clean up tables. Briefly describe the output that appears below the codeblock.
**Answer** a cleaned verson of those tibbles that lowercased the headers, and only looks at the "employment, all employyes (seasonally adjusted)" row

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row;
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
We now have 19 dataframes, each containing one row each and two columns, one of which is the employment number for a given sector for jun_2021. But we're missing information about what industry sector these employment numbers represent.

We can add that back in by using bind_cols() to reconnect the each_row_df, which contains the sector code and the sector name.

### Task 33: Run code to run for loop to add in data
**Task** Run the following for loop to add in data. Briefly describe the output that appears below the codeblock.
**Answer** 19 tibbles that combined the employment information table with the sectors table

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
Then we'll do a little bit of cleaning.

Let's use parse_number() to remove the comma from the jun_2021 number and convert it from a character to number. We'll use rename() to make the jun_2021 column name a little more descriptive. And then we'll use select() to keep only the columns we want to keep -- the sector number, the sector name, and the jun_2021 employment number.

### Task 34: Run code to run for loop to clean up tables
**Task** Run the following for loop to clean up tables. Briefly describe the output that appears below the codeblock.
**Answer** 19 tibbles containing the sector and sector description and number of employees for November 2021

```{r}

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jun_2021 column into a proper number, and rename it.  Then select only three columns we need.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df) %>%
      mutate(nov_2021 = parse_number(nov_2021)) %>%
      rename(nov_2021_employees = nov_2021) %>%
      select(sector,description,nov_2021_employees)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}

```


We're getting very close to the finished table we showed at the beginning.  

But right now, each bit of sector information is separated between 19 different dataframes.  

We want them in one dataframe.  

We can fix this by creating an empty dataframe called "employment_by_sector_all" using tibble(), placing it before our "for loop".

And inside our "for loop" at the end, we'll bind each employment_info dataframe to the newly created empty dataframe.  

### Task 35: Run code to run for loop to combine tables into a single table
**Task** Run the following for loop combine tables into a single table. Briefly describe the output that appears below the codeblock.
**Answer** a tibble of all the sectors and sector descriptions and their respective employment information for November 2021

```{r}

# Create an empty dataframe to hold results
employment_by_sector_all <- tibble()

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jun_2021 column into a proper number, and rename it.  Then select only three columns we need.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df) %>%
      mutate(nov_2021 = parse_number(nov_2021)) %>%
      rename(nov_2021_employees = nov_2021) %>%
      select(sector,description,nov_2021_employees)

    # Bind each individual employment info table to our employment_by_sector_all dataframe
    employment_by_sector_all <- employment_by_sector_all %>%
      bind_rows(employment_info)

}

# Display the completed dataframe
employment_by_sector_all
```

Ta da! The end result is a nice tidy dataframe with the number of employees in June 2021 for each sector.

It's always a good idea to spot check the results, especially any values that look suspiciously high or low.

The value for "Agriculture, Forestry, Fishing and Hunting" seems suspiciously low, compared with the other values.  

Let's figure out why.  

Here's the table on the mining sector page: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm)



In the second row of this table, it has the unemployment rate. Nowhere on the page can we find information on the number of employees.  We would need to do additional research to track down a valid number if we plan on using this table, but for now we're going to replace it with an NA using na_if.

### Task 40: Run code to remove agriculture from table
**Task** Run the following code to remove agriculture from table. Briefly describe the output that appears below the codeblock.
**Answer** This removes the odd number and replaces it with NA from the November 2021 employment in the agriculture sector

```{r}
# remove the suspicious value for agriculture.
employment_by_sector_all <- employment_by_sector_all %>%
  mutate(nov_2021_employees = na_if(nov_2021_employees,5.4))

# display it
employment_by_sector_all
```

And we're done.  

A note about advanced scraping -- every site is different. Every time you want to scrape a site, you'll be puzzling over different problems. But the steps remain the same: find a pattern, exploit it, clean the data on the fly and put it into a place to store it.
